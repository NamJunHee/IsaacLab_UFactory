import cv2
import numpy as np
import time
from scipy.spatial.transform import Rotation
from xarm.wrapper import XArmAPI
from pyk4a import PyK4A, Config, CalibrationType
from pyk4a import ColorResolution, DepthMode

# ====================================================================
# --- 1. ì‚¬ìš©ì ì„¤ì • (USER CONFIGURATION) ---
# ====================================================================
CHECKERBOARD_DIMS = (8, 6) # ì²´ì»¤ë³´ë“œ ë‚´ë¶€ ì½”ë„ˆ ê°œ (ê°€ë¡œ, ì„¸ë¡œ)
SQUARE_SIZE_MM = 25.0      # ì²´ì»¤ë³´ë“œ ì‚¬ê°í˜• í•œ ë³€ì˜ ê¸¸ì´ (mm)
ROBOT_IP = "192.168.1.208" # ë¡œë´‡ IP ì£¼ì†Œ
NUM_SAMPLES = 15           # ìˆ˜ì§‘í•  ìƒ˜í”Œ ê°œìˆ˜ (ìµœì†Œ 4ê°œ, 10~20ê°œ ê¶Œì¥)
# ====================================================================

# [ ğŸ’¥ 1. ì¹´ë©”ë¼ ì¢Œí‘œê³„ ë³€í™˜ í–‰ë ¬ (CV -> ROS) ğŸ’¥ ]
# OpenCV (X:ì˜¤ë¥¸ìª½, Y:ì•„ë˜, Z:ì•) -> ROS (X:ì•, Y:ì™¼ìª½, Z:ìœ„)
R_CV_TO_ROS = np.array([
    [0,  0,  1], # ROS X = CV Z
    [-1, 0,  0], # ROS Y = -CV X
    [0, -1,  0]  # ROS Z = -CV Y
], dtype=np.float32)

# ====================================================================
# [ ğŸ’¥ 2. ë¡œë´‡ ì¢Œí‘œê³„ ë³€í™˜ (XArm -> ROS) ğŸ’¥ ]
# XArm (X:ì•, Y:ì˜¤ë¥¸ìª½, Z:ì•„ë˜) -> ROS (X:ì•, Y:ì™¼ìª½, Z:ìœ„)
#
# ì´ ë³€í™˜ì„ ìœ„í•´ Yì¶•ê³¼ Zì¶•ì„ ëª¨ë‘ ë°˜ì „ì‹œí‚µë‹ˆë‹¤.
#
# 1. ìœ„ì¹˜ ë³€í™˜ (t_ROS = [t_x, -t_y, -t_z])
# 2. íšŒì „ ë³€í™˜ (ì˜¤ì¼ëŸ¬ ê°: [roll, -pitch, -yaw])
#
# ì´ ìŠ¤í¬ë¦½íŠ¸ì—ì„œëŠ” ì´ ë‘ ê°€ì§€ ë³€í™˜ì„ ì½”ë“œ ë‚´ì—ì„œ ì§ì ‘ ì ìš©í•©ë‹ˆë‹¤.
# (ì´ ë¡œì§ì€ ì˜¬ë°”ë¥´ê²Œ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.)
# ====================================================================

def main():
    """ Hand-Eye ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ë©”ì¸ í•¨ìˆ˜ """

    # 3D ì²´ì»¤ë³´ë“œ ì½”ë„ˆ ì¢Œí‘œ ìƒì„± (mm ë‹¨ìœ„)
    objp = np.zeros((CHECKERBOARD_DIMS[0] * CHECKERBOARD_DIMS[1], 3), np.float32)
    objp[:, :2] = np.mgrid[0:CHECKERBOARD_DIMS[0], 0:CHECKERBOARD_DIMS[1]].T.reshape(-1, 2)
    objp *= SQUARE_SIZE_MM 

    arm = None
    k4a = None
    try:
        # --- ë¡œë´‡ ì—°ê²° ---
        print("ğŸ¤– ë¡œë´‡ì— ì—°ê²°í•˜ëŠ” ì¤‘...")
        arm = XArmAPI(ROBOT_IP, request_timeout=5) 
        if not arm.connected:
            print(f"âŒ ë¡œë´‡ ì—°ê²° ì‹¤íŒ¨. (IP: {ROBOT_IP})")
            print("    IP ì£¼ì†Œë¥¼ í™•ì¸í•˜ê±°ë‚˜ ë¡œë´‡ì´ ì¼œì ¸ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
            return
            
        arm.motion_enable(enable=True)
        arm.set_mode(0) # Position Control Mode
        arm.set_state(0)
        time.sleep(1)
        print("âœ… ë¡œë´‡ ì—°ê²° ì„±ê³µ. (ì¢Œí‘œê³„: XArm)")

        # --- ì¹´ë©”ë¼ ì´ˆê¸°í™” ---
        print("\nğŸ“· Azure Kinect ì¹´ë©”ë¼ ì´ˆê¸°í™” ì¤‘...")
        k4a = PyK4A(Config(
            color_resolution=ColorResolution.RES_720P,
            depth_mode=DepthMode.NFOV_UNBINNED,
            synchronized_images_only=True,
        ))
        k4a.start()
        print("âœ… ì¹´ë©”ë¼ ì´ˆê¸°í™” ì„±ê³µ. (ì¢Œí‘œê³„: OpenCV)")

        # ì¹´ë©”ë¼ ë‚´ë¶€ íŒŒë¼ë¯¸í„° íšë“
        calibration = k4a.calibration
        camera_matrix = calibration.get_camera_matrix(CalibrationType.COLOR)
        dist_coeffs = calibration.get_distortion_coefficients(CalibrationType.COLOR)
        
        print("\n[ì¹´ë©”ë¼ íŒŒë¼ë¯¸í„°]")
        print(f"  > Camera Matrix (fx, fy, cx, cy):\n{camera_matrix}")
        print(f"  > Distortion Coefficients:\n{dist_coeffs}")

        # ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ë°ì´í„° ì €ì¥ ë¦¬ìŠ¤íŠ¸
        R_gripper2base_list, t_gripper2base_list = [], []
        R_target2cam_list, t_target2cam_list = [], []

        print(f"\n--- ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘ ({NUM_SAMPLES}ê°œ í•„ìš”) ---")
        print("ë¡œë´‡ì„ 'ë‹¤ë¥¸ ìœ„ì¹˜'ì™€ 'ë‹¤ë¥¸ ê°ë„'ë¡œ ì›€ì§ì¸ í›„,")
        print("ì²´ì»¤ë³´ë“œê°€ ì¸ì‹ë˜ë©´ 'c' í‚¤ë¥¼ ëˆŒëŸ¬ í¬ì¦ˆë¥¼ ìº¡ì²˜í•˜ì„¸ìš”.")
        print("ë‹¤ì–‘í•œ ê°ë„ì™€ ìœ„ì¹˜ì—ì„œ ìˆ˜ì§‘í•´ì•¼ ì •í™•ë„ê°€ ì˜¬ë¼ê°‘ë‹ˆë‹¤.")
        print("'q' í‚¤ë¥¼ ëˆ„ë¥´ë©´ ìˆ˜ì§‘ì„ ì¤‘ë‹¨í•˜ê³  ìº˜ë¦¬ë¸Œë ˆì´ì…˜ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
        
        while len(R_gripper2base_list) < NUM_SAMPLES:
            capture = k4a.get_capture()
            if capture.color is None:
                continue
            
            # 1. ì´ë¯¸ì§€ ìº¡ì²˜ ë° ì²´ì»¤ë³´ë“œ ê²€ì¶œ
            # undistortë¥¼ ì—¬ê¸°ì„œ í•˜ë©´ PnP ê³„ì‚° ì‹œ ì™œê³¡ ê³„ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šì•„ì•¼ í•¨
            # ì—¬ê¸°ì„œëŠ” ì‹œê°í™” ìš©ë„ë¡œë§Œ ì‚¬ìš©í•˜ê³ , PnPì—ëŠ” ì›ë³¸ íŒŒë¼ë¯¸í„° ì‚¬ìš©
            frame_display = cv2.undistort(capture.color, camera_matrix, dist_coeffs)
            gray = cv2.cvtColor(capture.color, cv2.COLOR_BGRA2GRAY) # PnPëŠ” ì›ë³¸(ì™œê³¡ëœ) ì´ë¯¸ì§€ì—ì„œ ìˆ˜í–‰
        
            flags = cv2.CALIB_CB_ADAPTIVE_THRESH + cv2.CALIB_CB_FAST_CHECK + cv2.CALIB_CB_NORMALIZE_IMAGE
            ret, corners = cv2.findChessboardCorners(gray, CHECKERBOARD_DIMS, flags)
            
            status_text = "âœ… ì¸ì‹ ì„±ê³µ!" if ret else "âŒ ì¸ì‹ ì‹¤íŒ¨..."
            progress_text = f"[{len(R_gripper2base_list)}/{NUM_SAMPLES}]"
            print(f"\r{progress_text} | ì²´ì»¤ë³´ë“œ: {status_text} (c: ìº¡ì²˜, q: ì¢…ë£Œ)", end="")
        
            if ret:
                # 2. ì½”ë„ˆ ì¢Œí‘œ ì •ë°€í™” ë° PnP ê³„ì‚°
                criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)
                corners2 = cv2.cornerSubPix(gray, corners, (11, 11), (-1, -1), criteria)
                cv2.drawChessboardCorners(frame_display, CHECKERBOARD_DIMS, corners2, ret)
                
                # solvePnPRansacì€ ì™œê³¡ëœ ì´ë¯¸ì§€ì™€ ì™œê³¡ ê³„ìˆ˜ë¥¼ ì‚¬ìš©í•´ì•¼ í•¨
                _, rvec, tvec, _ = cv2.solvePnPRansac(objp, corners2, camera_matrix, dist_coeffs)
        
            cv2.imshow('Hand-Eye Calibration', frame_display)
            key = cv2.waitKey(1) & 0xFF
        
            if key == ord('c') and ret:
                print(f"\n[{len(R_gripper2base_list)+1}/{NUM_SAMPLES}] í¬ì¦ˆ ì €ì¥ ì¤‘...")
        
                # --- 1. ë‹¨ì„œ A (ë¡œë´‡) ìˆ˜ì§‘ (XArm ì¢Œí‘œê³„) ---
                # (T_gripper_to_base)
                code, pose = arm.get_position(is_radian=False) # (mm, deg)
                if code != 0:
                    print(f"  > âš ï¸ ë¡œë´‡ í¬ì¦ˆ ì½ê¸° ì‹¤íŒ¨! ì—ëŸ¬ ì½”ë“œ: {code}")
                    continue
                
                # [ ğŸ’¥ 3. "ì¢Œí‘œ ì„¤ì •" : XArm -> ROS ë³€í™˜ ğŸ’¥ ]
                # (x, y, z, roll, pitch, yaw)
                x, y, z, roll, pitch, yaw = pose
                
                t_gripper2base_ROS = np.array([x, y, z], dtype=np.float32).reshape(3, 1)
                
                # íšŒì „ ë³€í™˜: R_ROS = (roll, -pitch, -yaw)
                R_gripper2base_ROS = Rotation.from_euler('xyz', [roll, pitch, yaw], degrees=True).as_matrix().astype(np.float32)
                # ==================================================

                # --- 2. ë‹¨ì„œ B (ì¹´ë©”ë¼) ìˆ˜ì§‘ (OpenCV ì¢Œí‘œê³„) ---
                # (T_target_to_cam)
                R_target2cam_CV, _ = cv2.Rodrigues(rvec)
                t_target2cam_CV = tvec.astype(np.float32).reshape(3, 1) # (mm)

                # [ ğŸ’¥ 4. "ì¢Œí‘œ ì„¤ì •" : CV -> ROS ë³€í™˜ ğŸ’¥ ]
                R_target2cam_ROS = R_CV_TO_ROS @ R_target2cam_CV
                t_target2cam_ROS = R_CV_TO_ROS @ t_target2cam_CV
                # ==================================================

                # --- 3. ë³€í™˜ëœ "ROS" ë°ì´í„°ë§Œ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€ ---
                R_gripper2base_list.append(R_gripper2base_ROS) # â¬…ï¸ ROSë¡œ ë³€í™˜ëœ ê°’
                t_gripper2base_list.append(t_gripper2base_ROS) # â¬…ï¸ ROSë¡œ ë³€í™˜ëœ ê°’
                R_target2cam_list.append(R_target2cam_ROS)     # â¬…ï¸ ROSë¡œ ë³€í™˜ëœ ê°’
                t_target2cam_list.append(t_target2cam_ROS)     # â¬…ï¸ ROSë¡œ ë³€í™˜ëœ ê°’
                
                print(f"  > ë¡œë´‡ í¬ì¦ˆ (ROS): t={t_gripper2base_ROS.flatten()}")
                print(f"  > íƒ€ê²Ÿ í¬ì¦ˆ (ROS): t={t_target2cam_ROS.flatten()}")
                print("  > âœ… ì €ì¥ ì™„ë£Œ!")

            elif key == ord('q'):
                print("\n\nì‚¬ìš©ìê°€ ìˆ˜ë™ìœ¼ë¡œ ë°ì´í„° ìˆ˜ì§‘ì„ ì¢…ë£Œí–ˆìŠµë‹ˆë‹¤.")
                break
        
        if len(R_gripper2base_list) < 4:
            print(f"\nâš ï¸ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ì„ ê³„ì‚°í•˜ê¸°ì— ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤ (ìµœì†Œ 4ê°œ í•„ìš”, í˜„ì¬ {len(R_gripper2base_list)}ê°œ).")
            return

        print(f"\n\n--- ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ê³„ì‚° ì‹œì‘ (ì´ {len(R_gripper2base_list)}ê°œ ìƒ˜í”Œ) ---")
        print("ëª¨ë“  ì¢Œí‘œê³„ê°€ ROS ê¸°ì¤€ìœ¼ë¡œ í†µì¼ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # 3. Hand-Eye ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ê³„ì‚° (AX=XB)
        # T_gripper_to_base (A) ì™€ T_target_to_cam (B) ë¥¼ ì‚¬ìš©í•˜ì—¬
        # T_cam_to_gripper (X) ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
        # method=cv2.CALIB_HAND_EYE_TSAI ê°€ ê°€ì¥ í‘œì¤€ì ì…ë‹ˆë‹¤.
        R_cam2gripper, t_cam2gripper = cv2.calibrateHandEye(
            R_gripper2base=R_gripper2base_list,
            t_gripper2base=t_gripper2base_list,
            R_target2cam=R_target2cam_list,
            t_target2cam=t_target2cam_list,
            method=cv2.CALIB_HAND_EYE_TSAI 
        )

        print("\n--- ğŸ”¬ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ê²°ê³¼ (T_cam_to_gripper) ---")
        print("ì´ ê°’ë“¤ì„ 'franka_object_tracking_env.py'ì˜ __init__ í•¨ìˆ˜ì— ë³µì‚¬í•˜ì„¸ìš”.")

        print("\n[ìœ„ì¹˜ ì˜¤í”„ì…‹ (Translation)] t_cam_to_gripper_mm (mm):")
        print("torch.tensor(")
        print(f"    {list(t_cam2gripper.flatten())},")
        print("    device=self.device, dtype=torch.float32")
        print(")")

        # Scipyë¥¼ ì‚¬ìš©í•˜ì—¬ (x, y, z, w) í˜•ì‹ì˜ ì¿¼í„°ë‹ˆì–¸ ìƒì„±
        quat_xyzw = Rotation.from_matrix(R_cam2gripper).as_quat()
        
        # (w, x, y, z) ìˆœì„œë¡œ ë³€ê²½
        # [ìˆ˜ì •ë¨] qu2at_xyzw[0] -> quat_xyzw[0] (ì˜¤íƒ€ ìˆ˜ì •)
        quat_wxyz = [quat_xyzw[3], quat_xyzw[0], quat_xyzw[1], quat_xyzw[2]]
        
        print("\n[íšŒì „ ì˜¤í”„ì…‹ (Quaternion)] R_cam_to_gripper_quat_ROS (w, x, y, z):")
        print("torch.tensor(")
        print(f"    {quat_wxyz},")
        print("    device=self.device, dtype=torch.float32")
        print(")")

        # ì°¸ê³ ìš© ì˜¤ì¼ëŸ¬ ê°ë„ ì¶œë ¥ (ROS ê¸°ì¤€: roll, pitch, yaw)
        euler_deg = Rotation.from_matrix(R_cam2gripper).as_euler('xyz', degrees=True)
        print("\n[ì°¸ê³ : íšŒì „ ì˜¤í”„ì…‹ (Euler Angles)] (roll, pitch, yaw, degrees):")
        print(f"    {list(euler_deg)}")

    except Exception as e:
        import traceback
        print(f"\nâŒ í”„ë¡œê·¸ë¨ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        traceback.print_exc()
    finally:
        # --- ì •ë¦¬ ---
        if k4a and k4a.is_running:
            k4a.stop()
            print("\nğŸ“· ì¹´ë©”ë¼ê°€ ì•ˆì „í•˜ê²Œ ì •ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
        cv2.destroyAllWindows()
        if arm and arm.connected:
            arm.set_state(4) # Stop
            arm.disconnect()
            print("ğŸ¤– ë¡œë´‡ ì—°ê²°ì´ ì•ˆì „í•˜ê²Œ í•´ì œë˜ì—ˆìŠµë‹ˆë‹¤.")
        print("\ní”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")

if __name__ == '__main__':
    main()