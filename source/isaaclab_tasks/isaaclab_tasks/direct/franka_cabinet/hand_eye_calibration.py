import cv2
import numpy as np
import time
from scipy.spatial.transform import Rotation
from xarm.wrapper import XArmAPI
from pyk4a import PyK4A, Config, CalibrationType
from pyk4a import ColorResolution, DepthMode

# ====================================================================
# --- 1. ì‚¬ìš©ì ì„¤ì • (USER CONFIGURATION) ---
# ====================================================================
CHECKERBOARD_DIMS = (8, 6) 
SQUARE_SIZE_MM = 25.0
ROBOT_IP = "192.168.1.208"
NUM_SAMPLES = 15
# ====================================================================

# [ ğŸ’¥ 1. ì¹´ë©”ë¼ ì¢Œí‘œê³„ ë³€í™˜ í–‰ë ¬ (CV -> ROS) ğŸ’¥ ]
# (X:ìš°, Y:í•˜, Z:ì „) -> (X:ì „, Y:ì¢Œ, Z:ìƒ)
R_CV_TO_ROS = np.array([
    [0,  0,  1],
    [-1, 0,  0],
    [0, -1,  0]
], dtype=np.float32)

# ====================================================================
# [ ğŸ’¥ 2. ë¡œë´‡ ì¢Œí‘œê³„ ë³€í™˜ í–‰ë ¬ (XArm -> ROS) ğŸ’¥ ]
# XArm (X:ì „, Y:ìš°, Z:ìƒ) -> ROS (X:ì „, Y:ì¢Œ, Z:ìƒ)
# ì´ ë³€í™˜ì€ T_gripper2base ì „ì²´ì— ì ìš©ë©ë‹ˆë‹¤.
# T_ROS = T_fix @ T_XArm @ inv(T_fix)
R_XARM_TO_ROS_FIX = np.array([
    [1,  0,  0],
    [0, -1,  0], # Yì¶• ë°˜ì „
    [0,  0, -1]  # Zì¶• ë°˜ì „ (Euler ë³€í™˜ ë°©ì‹ ë•Œë¬¸ì— í•„ìš”)
])
# ì˜¤ì¼ëŸ¬ ê° ë³€í™˜ì„ ìœ„í•œ ë³´ì • (Y, Z ë¶€í˜¸ ë°˜ì „)
# t_ROS = T_fix @ t_XArm
# R_ROS = R_fix @ R_XArm @ R_fix_inv
# (XArm_Y -> -ROS_Y), (XArm_Z -> ROS_Z)
# (XArm_roll -> ROS_roll), (XArm_pitch -> -ROS_pitch), (XArm_yaw -> -ROS_yaw)
# ====================================================================

def main():
    """ Hand-Eye ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ë©”ì¸ í•¨ìˆ˜ """

    objp = np.zeros((CHECKERBOARD_DIMS[0] * CHECKERBOARD_DIMS[1], 3), np.float32)
    objp[:, :2] = np.mgrid[0:CHECKERBOARD_DIMS[0], 0:CHECKERBOARD_DIMS[1]].T.reshape(-1, 2)
    objp *= SQUARE_SIZE_MM # (mm ë‹¨ìœ„)

    arm = None
    k4a = None
    try:
        print("ğŸ¤– ë¡œë´‡ì— ì—°ê²°í•˜ëŠ” ì¤‘...")
        arm = XArmAPI(ROBOT_IP, request_timeout=5) 
        arm.motion_enable(enable=True)
        arm.set_mode(0)
        arm.set_state(0)
        time.sleep(1)
        if not arm.connected:
            print("âŒ ë¡œë´‡ ì—°ê²° ì‹¤íŒ¨.")
            return
        print("âœ… ë¡œë´‡ ì—°ê²° ì„±ê³µ. (ì¢Œí‘œê³„: XArm)")

        print("\nğŸ“· Azure Kinect ì¹´ë©”ë¼ ì´ˆê¸°í™” ì¤‘...")
        k4a = PyK4A(Config(
            color_resolution=ColorResolution.RES_720P,
            depth_mode=DepthMode.NFOV_UNBINNED,
            synchronized_images_only=True,
        ))
        k4a.start()
        print("âœ… ì¹´ë©”ë¼ ì´ˆê¸°í™” ì„±ê³µ. (ì¢Œí‘œê³„: OpenCV)")

        calibration = k4a.calibration
        camera_matrix = calibration.get_camera_matrix(CalibrationType.COLOR)
        dist_coeffs = calibration.get_distortion_coefficients(CalibrationType.COLOR)

        R_gripper2base_list, t_gripper2base_list = [], []
        R_target2cam_list, t_target2cam_list = [], []

        print(f"\n--- ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘ ({NUM_SAMPLES}ê°œ í•„ìš”) ---")
        
        while len(R_gripper2base_list) < NUM_SAMPLES:
            capture = k4a.get_capture()
            if capture.color is None:
                continue
            
            frame = cv2.undistort(capture.color, camera_matrix, dist_coeffs)
            gray = cv2.cvtColor(frame, cv2.COLOR_BGRA2GRAY)
        
            flags = cv2.CALIB_CB_ADAPTIVE_THRESH + cv2.CALIB_CB_FAST_CHECK + cv2.CALIB_CB_NORMALIZE_IMAGE
            ret, corners = cv2.findChessboardCorners(gray, CHECKERBOARD_DIMS, flags)
            
            status_text = "âœ… ì¸ì‹ ì„±ê³µ!" if ret else "âŒ ì¸ì‹ ì‹¤íŒ¨..."
            progress_text = f"[{len(R_gripper2base_list)}/{NUM_SAMPLES}]"
            print(f"\r{progress_text} | ì²´ì»¤ë³´ë“œ: {status_text}", end="")
        
            if ret:
                criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)
                corners2 = cv2.cornerSubPix(gray, corners, (11, 11), (-1, -1), criteria)
                cv2.drawChessboardCorners(frame, CHECKERBOARD_DIMS, corners2, ret)
                _, rvec, tvec, _ = cv2.solvePnPRansac(objp, corners2, camera_matrix, dist_coeffs)
        
            cv2.imshow('Hand-Eye Calibration', frame)
            key = cv2.waitKey(1) & 0xFF
        
            if key == ord('c') and ret:
                print(f"\n[{len(R_gripper2base_list)+1}/{NUM_SAMPLES}] í¬ì¦ˆ ì €ì¥ ì¤‘...")
        
                # --- 1. ë‹¨ì„œ A (ë¡œë´‡) ìˆ˜ì§‘ (XArm ì¢Œí‘œê³„) ---
                code, pose = arm.get_position(is_radian=False)
                # print("robot_pose:")
                if code != 0:
                    print(f"  > âš ï¸ ë¡œë´‡ í¬ì¦ˆ ì½ê¸° ì‹¤íŒ¨! ì—ëŸ¬ ì½”ë“œ: {code}")
                    continue
                
                # [ ğŸ’¥ 3. "ì¢Œí‘œ ì„¤ì •" ìˆ˜ì •: XArm -> ROS ë³€í™˜ ğŸ’¥ ]
                # (x, y, z, roll, pitch, yaw)
                x, y, z, roll, pitch, yaw = pose
                
                # XArm(Y-Right) -> ROS(Y-Left) ë³€í™˜
                t_gripper2base_ROS = np.array([x, -y, z]).reshape(3, 1) # Yì¶• ë¶€í˜¸ ë°˜ì „
                
                # íšŒì „ ë³€í™˜: R_ROS = R_fix @ R_XArm
                # XArmì˜ (roll, pitch, yaw)ëŠ” 'xyz' ìˆœì„œ
                R_gripper2base_XArm = Rotation.from_euler('xyz', [roll, pitch, yaw], degrees=True).as_matrix()
                
                # R_ROS = R_fix @ R_XArm @ R_fix_inv (Tait-Bryan ZYX convention)
                # ë” ê°„ë‹¨í•œ ì˜¤ì¼ëŸ¬ ê° ë³€í™˜: (roll, -pitch, -yaw)
                R_gripper2base_ROS = Rotation.from_euler('xyz', [roll, -pitch, -yaw], degrees=True).as_matrix()
                # ==================================================

                # --- 2. ë‹¨ì„œ B (ì¹´ë©”ë¼) ìˆ˜ì§‘ (OpenCV ì¢Œí‘œê³„) ---
                R_target2cam_CV, _ = cv2.Rodrigues(rvec)
                t_target2cam_CV = tvec.reshape(3, 1)

                # [ ğŸ’¥ 4. "ì¢Œí‘œ ì„¤ì •" ìˆ˜ì •: CV -> ROS ë³€í™˜ ğŸ’¥ ]
                # (ì´ ë¶€ë¶„ì€ ì´ì „ê³¼ ë™ì¼í•˜ê²Œ ì˜¬ë°”ë¦„)
                R_target2cam_ROS = R_CV_TO_ROS @ R_target2cam_CV
                t_target2cam_ROS = R_CV_TO_ROS @ t_target2cam_CV
                # ==================================================

                # --- 3. ë³€í™˜ëœ "ROS" ë°ì´í„°ë§Œ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€ ---
                R_gripper2base_list.append(R_gripper2base_ROS) # â¬…ï¸ ROSë¡œ ë³€í™˜ëœ ê°’
                t_gripper2base_list.append(t_gripper2base_ROS) # â¬…ï¸ ROSë¡œ ë³€í™˜ëœ ê°’
                R_target2cam_list.append(R_target2cam_ROS)     # â¬…ï¸ ROSë¡œ ë³€í™˜ëœ ê°’
                t_target2cam_list.append(t_target2cam_ROS)     # â¬…ï¸ ROSë¡œ ë³€í™˜ëœ ê°’
                
                print("  > âœ… ì €ì¥ ì™„ë£Œ!")

            elif key == ord('q'):
                print("\n\nì‚¬ìš©ìê°€ ìˆ˜ë™ìœ¼ë¡œ ë°ì´í„° ìˆ˜ì§‘ì„ ì¢…ë£Œí–ˆìŠµë‹ˆë‹¤.")
                break
        
        if len(R_gripper2base_list) < 4:
            print("\nâš ï¸ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ì„ ê³„ì‚°í•˜ê¸°ì— ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
            return

        print("\n\n--- ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ê³„ì‚° ì‹œì‘ (ëª¨ë“  ì¢Œí‘œ ROS í†µì¼ë¨) ---")
        
        # ì´ì œ (ROS, ROS, ROS, ROS) ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ ê²°ê³¼ê°€ ì˜¬ë°”ë¦…ë‹ˆë‹¤.
        R_cam2gripper, t_cam2gripper = cv2.calibrateHandEye(
            R_gripper2base=R_gripper2base_list,
            t_gripper2base=t_gripper2base_list,
            R_target2cam=R_target2cam_list,
            t_target2cam=t_target2cam_list,
            method=cv2.CALIB_HAND_EYE_TSAI
        )

        print("\n--- ğŸ”¬ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ê²°ê³¼ (ROS ì¢Œí‘œê³„ ê¸°ì¤€) ---")
        print("[ìœ„ì¹˜ ì˜¤í”„ì…‹ (Translation)] T_cam_in_gripper (mm):")
        print(t_cam2gripper.flatten())

        quat_xyzw = Rotation.from_matrix(R_cam2gripper).as_quat()
        quat_wxyz = [quat_xyzw[3], quat_xyzw[0], quat_xyzw[1], quat_xyzw[2]]
        print("\n[íšŒì „ ì˜¤í”„ì…‹ (Quaternion)] Q_cam_in_gripper (w, x, y, z):")
        print(np.array(quat_wxyz))

        euler_deg = Rotation.from_matrix(R_cam2gripper).as_euler('xyz', degrees=True)
        print("\n[íšŒì „ ì˜¤í”„ì…‹ (Euler Angles)] R_cam_in_gripper (roll, pitch, yaw, degrees):")
        print(euler_deg)

    except Exception as e:
        print(f"\nâŒ í”„ë¡œê·¸ë¨ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    finally:
        if k4a and k4a.is_running:
            k4a.stop()
            print("\nğŸ“· ì¹´ë©”ë¼ê°€ ì•ˆì „í•˜ê²Œ ì •ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
        cv2.destroyAllWindows()
        if arm and arm.connected:
            arm.disconnect()
            print("ğŸ¤– ë¡œë´‡ ì—°ê²°ì´ ì•ˆì „í•˜ê²Œ í•´ì œë˜ì—ˆìŠµë‹ˆë‹¤.")
        print("\ní”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")

if __name__ == '__main__':
    main()